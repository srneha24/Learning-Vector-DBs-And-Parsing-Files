{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from dotenv import load_dotenv\n",
    "from gensim.models import Word2Vec\n",
    "from chromadb import PersistentClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ./nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ./nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data files\n",
    "\n",
    "nltk.download('punkt', download_dir='./nltk')\n",
    "nltk.download('stopwords', download_dir='./nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare variables\n",
    "\n",
    "sample_file_path = \"samples\"\n",
    "model_name = \"sample.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files and create tokens\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "doc_tokens = list()\n",
    "doc_content = list()\n",
    "for sample_file in os.listdir(\"./\" + sample_file_path):\n",
    "    with open(f\"./{sample_file_path}/{sample_file}\", 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        doc_content.append(content)\n",
    "        tokens = word_tokenize(content)\n",
    "        doc_tokens.append([word.lower() for word in tokens if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=681, vector_size=100, alpha=0.025>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11123, 15060)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "model = Word2Vec(sentences=doc_tokens, min_count=1, window=5, workers=4)\n",
    "print(model)\n",
    "model.train(doc_tokens, total_examples=len(doc_tokens), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Word2Vec`\n",
    "\n",
    "**sentences:** The list of sentences split into words in lowercase.\n",
    "\n",
    "**min_count:** Which words to consider in accordance to the number of times they appear in the sentences. For example, if set to 1, that means all the words that occur once or more in all of the sentences will be used to create the embeddings. If set to 2, then all the words that occur twice or more will be created embeddings for.\n",
    "\n",
    "**window:** The maximum distance between the current and predicted word within a sentence. That is, how many words to the left and right of a given word are considered when training the model.\n",
    "\n",
    "**workers:** How many CPU cores will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document embeddings\n",
    "\n",
    "doc_embeddings = list()\n",
    "for doc_token in doc_tokens:\n",
    "    valid_tokens = [token for token in doc_token if token in model.wv]\n",
    "    if not valid_tokens:\n",
    "        vector = np.zeros(model.vector_size)\n",
    "    else:\n",
    "        vector = np.mean([model.wv[token] for token in valid_tokens], axis=0)\n",
    "    doc_embeddings.append([float(value) for value in vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insantiate Chroma DB client\n",
    "\n",
    "chroma_client = PersistentClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and populate collection\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"document_embeddings\")\n",
    "\n",
    "for idx, embedding in enumerate(doc_embeddings):\n",
    "    collection.upsert(\n",
    "        ids=[\"doc1\", \"doc2\", \"doc3\"],\n",
    "        embeddings=doc_embeddings,\n",
    "        documents=doc_content\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_ORGANIZATION_ID = os.getenv('OPENAI_ORGANIZATION_ID')\n",
    "OPENAI_PROJECT_ID = os.getenv('OPENAI_PROJECT_ID')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
